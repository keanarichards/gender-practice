---
title: "Pilot study methods"
author: "Keana Richards"
date: "6/9/2020"
output:
  word_document: default
---

```{r, include = F}

## hiding code chunks globally 
knitr::opts_chunk$set(echo=FALSE, message = F, warning = F)


## limiting number of digits to 2 decimal places
options(digits = 2)


# load packages -----------------------------------------------------------
## Package names
packages <- c("readr", "tidyverse", "here", "summarytools")

## Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

## Packages loading
invisible(lapply(packages, library, character.only = TRUE))



clean <- read_csv(here("pilot", "data", "clean.csv"))
excluded <- read_csv(here("pilot", "data", "excluded.csv"))
dropped_out <- read_csv(here("pilot", "data", "dropped_out.csv"))
```

We recruited workers on Amazon Mechanical Turk for a study on decision-making. The workers who opted into the study had to pass several screening questions to be included as participants in the paid portion of the study. Specifically, participants included in the study had to (i) identify their nationality as American and live in the United States, (ii) identify as male or female, (iii) be using a computer (rather than a phone or tablet), and (iv) pass three comprehension questions about the task they would be completing. If they did not meet these criteria, they did not proceed to the paid portion of the study. Additionally, upon reviewing the data, we found that certain participants had a duplicate IP address, MTurkID, and gender, so we excluded the second response from those participants. Based on these criteria, `r nrow(excluded)` participants were excluded from the analyses. Thus, the final sample consisted of `r nrow(clean)` participants (`r freq(clean$gender, report.nas = F)[6]`% women), with an average age of `r mean(clean$age)` (*SD* = `r sd(clean$age)`) years. Of this final sample, `r nrow(dropped_out)` participants (`r freq(dropped_out$Gender, report.nas = F)[5]`% women) dropped out of the study before finishing (*M* = `r mean(dropped_out$Age)` years of age, *SD* = `r sd(dropped_out$Age)`). We include their data in analyses when available. 

Participants were told that they would complete three rounds of a key-entry task where they would solve as many problems as they could within one minute and be paid based on their performance. The task required participants to use a legend to enter letters associated with a series of 2-digit numbers shown on the screen. The legend consisted of five numbers with their corresponding letters below them. The letters were randomly generated, and the legend was the same across all participants and rounds. For instance, if the letter "C" was associated with the number 1 and "R" was associated with the number 2, and participants were presented with the number 12, they would have to enter "CR" into a corresponding text box. Before the paid rounds, participants were shown an example problem with the correct answer. Following this, they were required to pass 3 practice problems to test their comprehension. Each of these problems was identical in structure to the problems they would receive during the paid rounds. 

In the first and second round of the task, participants were incentivized according to a piece-rate and tournament payment scheme, respectively [@Niederle2007]. Under the piece-rate scheme, participants were told that they would earn \$.05 for each correct answer. Under the tournament payment scheme, participants were told that they would earn \$.10 for each correct answer, but were only paid if their score was greater than the second-round performance of another anonymous, randomly matched opponent who was doing the same experiment with the same sequence of numbers. In the third round, participants could choose between the two above payment schemes. We denote this decision as "choice to compete" [@Niederle2007]. 

After the third round, participants completed a series of follow-up questions which measured confidence, perceptions of gender differences in performance on the task, and risk aversion. 

We adapted previous measures of confidence [i.e., @Niederle2007] in our study by asking participants to (i) indicate whether they thought their round 2 score was higher or lower than the person they had competed against and (ii, iii) predict which decile their score might fall into relative to all men (women) who completed the task during round 2. To measure perceptions of gender differences, we asked participants to indicate whether they thought "women or men generally do better in the key-entry task" that they completed. We also asked if they thought their score would have improved if they practiced the task beforehand. Finally, we asked if they would they have taken the opportunity to practice the task, if offered. If they responded yes, they were asked how long they would have practiced (in minutes) if they were given unlimited time to practice. Participants were told that if they answered one of the confidence and perceptions of gender differences questions correctly they could receive a bonus. Specifically, we randomly selected one of their responses and if the selected prediction was correct, they received a bonus of \$.10. 

Finally, we measured risk aversion by asking participants "How do you see yourself: Are you generally a person who is fully prepared to take risks or do you try to avoid taking risks?" [@Dohmen2011b] on a 10 point scale, where 0 indicates participants are "Not at all willing to take risks," while 10 indicates participants are "very willing to take risks." 