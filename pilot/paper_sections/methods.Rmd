---
title: "Pilot study methods"
author: "Keana Richards"
date: "6/9/2020"
output:
  word_document: default
---

```{r, include = F}

## hiding code chunks globally 
knitr::opts_chunk$set(echo=FALSE, message = F, warning = F)


## limiting number of digits to 2 decimal places
options(digits = 2)


# load packages -----------------------------------------------------------
## Package names
packages <- c("readr", "tidyverse", "here", "summarytools")

## Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

## Packages loading
invisible(lapply(packages, library, character.only = TRUE))



clean <- read_csv(here("pilot", "data", "clean.csv"))
excluded <- read_csv(here("pilot", "data", "excluded.csv"))
dropped_out <- read_csv(here("pilot", "data", "dropped_out.csv"))


```

We recruited workers on Amazon Mechanical Turk for a study on decision-making. The workers who opted into the study had to pass several screening questions to be included as participants in the paid portion of the study. Specifically, participants included in the study had to (i) identify their nationality as American and live in the United States, (ii) identify as male or female, (iii) be using a computer (rather than a phone or tablet), and (iv) pass three comprehension questions about the task they would be completing. If they did not meet these criteria, they did not proceed to the paid portion of the study. Additionally, upon reviewing the data, we had reason to suspect that some participants completed the study more than once. Specifically, some participants had the same IP address, MTurk ID, and were of the same gender. When entries matched on all three identifiers, we included only the first entry and excluded all subsequent entries. The final sample consisted of `r nrow(clean)` participants (`r freq(clean$gender, report.nas = F)[6]`% women), with an average age of `r mean(clean$age)` (*SD* = `r sd(clean$age)`) years. `r nrow(dropped_out)` participants (`r freq(dropped_out$Gender, report.nas = F)[5]`% women) dropped out of the study before finishing. We include their data in analyses when available. 

Participants were told that they would complete three rounds of a key-entry task where they would solve as many problems as they could within one minute and be paid based on their performance. The task required participants to use a legend to enter letters associated with a series of 2-digit numbers shown on the screen. The legend consisted of five numbers with a corresponding letter below each number. The letters used were randomly generated. However, all participants saw the same legend and the legend did not change across rounds. For instance, the letter "Z" was associated with the number 1 and "A" was associated with the number 2. If participants were presented with the problem "1 2", they would have to enter "C R" into the corresponding text box. Before the paid rounds, participants were shown an example problem with the correct answer. Following this, they were required to pass three practice problems to test their comprehension. Each of these problems was identical in structure to the problems they would receive during the paid rounds. 

In the first and second round of the task, participants were incentivized according to a piece-rate and tournament payment scheme, respectively [@Niederle2007]. Under the piece-rate scheme, participants were told that they would earn \$.05 for each correct answer. Under the tournament payment scheme, participants were told that they would earn \$.10 for each correct answer, but were only paid if their score was greater than the second-round performance of another anonymous, randomly matched opponent who was doing the same experiment with the same sequence of numbers. In the third round, participants were asked to choose between the two above payment schemes. We denote this decision as "choice to compete" [@Niederle2007]. 

After the third round, participants completed a series of follow-up questions which measured confidence, perceptions of gender differences in performance on the task, and risk aversion. 

We adapted previous measures of confidence [i.e., @Niederle2007] in our study by asking participants to (i) indicate whether they thought their round 2 score was higher or lower than the person they had competed against and (ii, iii) predict which decile their round 2 score will fall into relative to all men (women) who completed the task during round 2. To measure perceptions of gender differences, we asked participants to indicate whether they thought "women or men generally do better in the key-entry task" that they completed. These answers were incentivized: participants were told that if they answered one of these questions correctly they could receive a bonus. Specifically,  participants were told that one of their responses would be randomly selected for payment and they would receive a bonus of $.10 if their answer was correct. 

Participants were asked whether they thought their score would have improved if they practiced the task beforehand. Additionally, we asked participants whether they would have practiced the task, if given the opportunity. If participants responded yes, they were asked how long they would have practiced for (in minutes) if they were given unlimited time.

Finally, we measured risk aversion by asking participants "How do you see yourself: Are you generally a person who is fully prepared to take risks or do you try to avoid taking risks?" [@Dohmen2011b] on a 10 point scale, where 0 indicates participants are "Not at all willing to take risks," while 10 indicates participants are "Very willing to take risks." 