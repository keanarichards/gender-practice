---
title: "Study 4 methods"
author: "Keana Richards"
output:
  word_document: default
---

```{r, include = F}

## hiding code chunks globally 
knitr::opts_chunk$set(echo=FALSE, message = F, warning = F)


## limiting number of digits to 2 decimal places
options(digits = 2)
options(pillar.sigfig = 2)
options(repos="https://cran.rstudio.com" )
# load packages -----------------------------------------------------------
## Package names
packages <- c("readr", "tidyverse", "here", "summarytools")

## Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

## Packages loading
invisible(lapply(packages, library, character.only = TRUE))

clean <- read_csv(here("study4", "data", "clean.csv"))
excluded <- read_csv(here("study4", "data", "excluded.csv"))
dropped_out <- read_csv(here("study4", "data", "dropped_out.csv"))

```

Participants were recruited on Amazon Mechanical Turk using the same screening criteria as Studies 1 and 2. Unlike Study 2, where we filtered out second responses based on identical IP addresses, we used Qualtricsâ€™ fraud detection software to filter out responses that were suspicious either because they were likely 1) bots and/or 2) duplicate responses. For all main analyses, we will exclude participants who have 1) Q_RecaptchaScore less than .5 (indicating the respondent is likely a bot) 2) Q_RelevantIDDuplicate equal to 1 (indicating the response is likely a duplicate) 3) Q_RelevantIDDuplicateScore greater than or equal to 75 (indicating the response is likely a duplicate) or 4) Q_RelevantIDFraudScore is greater than or equal to 30 (indicating the response is likely fraudulent and a bot). 

The final dataset consists of `r nrow(clean)` participants (`r freq(clean$gender, report.nas = F)[6]`% women), with an average age of `r mean(clean$age, na.rm = T)` (*SD* = `r sd(clean$age, na.rm = T)`) years. Of the final sample, `r nrow(dropped_out)` participants (`r freq(dropped_out$Gender...17, report.nas = F)[5]`% women) dropped out of the study before finishing and `r nrow(clean %>% filter(fraud ==1))` participants were flagged by Qualtrics' fraud detection software as suspicious based on the aforementioned criteria. We include analyses for the full sample in the appendix and all results are unchanged. We will compare the effects while performing the pre-registered analyses both with and without the participants that were flagged by Qualtrics to ensure that the results hold despite possibly fraudulent responses.

As in Studies 1 and 2, participants included in the study were told they would be completing a two-minute multiplication task (identical to the ones used in previous studies) and would be able to choose a payment scheme for their performance. After being told about the rules for the multiplication task and passing the same comprehension questions used in the previous studies, participants were assigned to either an unlimited preparation condition, where they could complete as many practice multiplication problems as they want, with the option to opt out of the practice at any time before moving on to the multiplication task, or a control condition, where they were told they could complete as many rounds of a subtraction exercise as they wanted before the multiplication task. An equal number of participants were randomly assigned to both conditions (control= `r freq(clean$condition, report.nas = F)[5]`%), with even representation of men and women across conditions, confirming there was random assignment to conditions based on gender. Participants across both conditions were given the option to study the multiplication (preparation condition) or subtraction (control condition) tables for as long as they wanted. We measured both the decision to study the respective table within each condition, along with the amount of time that participants who chose to study the tables spent on that page. Next, participants within each condition were given the option to practice problems from the tables within their respective condition. Practice problems were created by randomly drawing pairs of numbers from 1 to 12. Participants across both conditions were able to complete 10 problems at a time before being prompted to indicate whether they would like to continue completing problems. Like the previous studies, we measured the decision to practice multiplication problems (or in the case of the control condition, complete subtraction problems), along with the number of rounds participants completed and number of questions answered (as a proxy for number of problems practiced). 

Afterwards, all participants chose a payment scheme for the multiplication task, described and counterbalanced in the same way as in Studies 1 and 2. Then, participants completed the paid multiplication task for two minutes. We included many of the same follow-up questions as in Studies 1 and 2, including measures of risk attitudes, confidence, and perceptions of gender differences in preparation (for participants in the preparation condition), competitiveness, and performance. Like before, participants were incentivized to answer the questions about their confidence and perceptions of gender differences correctly, and were paid at the same rate as Studies 1 and 2. Participants also completed a manipulation check, where they were told about the two conditions, and were asked which of the conditions they thought was more helpful in boosting scores on the paid multiplication task. Finally, they completed some demographic questions and provided feedback on the study before being paid for their participation. 