---
title: "Study 4 methods"
author: "Keana Richards"
output:
  word_document: default
---

```{r, include = F}

## hiding code chunks globally 
knitr::opts_chunk$set(echo=FALSE, message = F, warning = F)


## limiting number of digits to 2 decimal places
options(digits = 2)
options(pillar.sigfig = 2)
options(repos="https://cran.rstudio.com" )
# load packages -----------------------------------------------------------
## Package names
packages <- c("readr", "tidyverse", "here", "summarytools", "papaja")

## Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

## Packages loading
invisible(lapply(packages, library, character.only = TRUE))

clean <- read_csv(here("study4", "data", "clean.csv"))
excluded <- read_csv(here("study4", "data", "excluded.csv"))
dropped_out <- read_csv(here("study4", "data", "dropped_out.csv"))

clean_fraud_removed <- clean %>% filter(fraud ==0)

source(here("study4", "source", "03_exploratory-analyses.R"))

```


All studies measures described below are publicly available on OSF both as a [.pdf](https://osf.io/yfd2j/) and [.qsf](https://osf.io/vhzy5/). Participants were recruited on Amazon Mechanical Turk using the same screening criteria as Studies 1 and 2. Unlike Study 2, where we filtered out second responses based on identical IP addresses, we used Qualtricsâ€™ fraud detection software to filter out responses that were suspicious either because they were likely 1) bots and/or 2) duplicate responses. For all main analyses, we excluded participants who had 1) Q_RecaptchaScore less than .5 (indicating the respondent is likely a bot) 2) Q_RelevantIDDuplicate equal to 1 (indicating the response is likely a duplicate) 3) Q_RelevantIDDuplicateScore greater than or equal to 75 (indicating the response is likely a duplicate) or 4) Q_RelevantIDFraudScore is greater than or equal to 30 (indicating the response is likely fraudulent and a bot). 

The final dataset consists of `r nrow(clean_fraud_removed)` participants (`r freq(clean_fraud_removed$gender, report.nas = F)[6]`% women), with an average age of `r mean(clean_fraud_removed$age, na.rm = T)` (*SD* = `r sd(clean_fraud_removed$age, na.rm = T)`) years. Of the final sample, `r nrow(dropped_out)` participants (`r freq(dropped_out$Gender...17, report.nas = F)[5]`% women) dropped out of the study before finishing and `r nrow(clean %>% filter(fraud ==1))` participants were flagged by Qualtrics' fraud detection software as suspicious based on the aforementioned criteria. We include analyses for the full sample in the appendix and all results are unchanged. As in Studies 1 and 2, participants included in the study were told they would be completing a two-minute multiplication task (identical to the ones used in previous studies) and would be able to choose a payment scheme for their performance. After being told about the rules for the multiplication task and passing the same comprehension questions used in the previous studies, participants were assigned to either an unlimited preparation condition, where they could complete as many practice multiplication problems as they want, with the option to opt out of the practice at any time before moving on to the multiplication task, or a control condition, where they were told they could complete as many rounds of a subtraction exercise as they wanted before the multiplication task. An equal number of participants were randomly assigned to both conditions (control= `r freq(clean_fraud_removed$condition, report.nas = F)[5]`%), with no significant difference in representation of men and women across conditions, `r apa_print(sec_exploratory38, n = nrow(clean_fraud_removed))$statistic`, confirming there was random assignment to conditions based on gender. Participants across both conditions were given the option to study the multiplication (preparation condition) or subtraction (control condition) tables for as long as they wanted. We measured both the decision to study the respective table within each condition, along with the amount of time that participants who chose to study the tables spent on that page. Next, participants within each condition were given the option to practice problems from the tables within their respective condition. Practice problems were created by randomly drawing pairs of numbers from 1 to 12 and asking participants to multiply them together. Participants across both conditions were able to complete 10 problems at a time before being prompted to indicate whether they would like to continue completing problems. Like the previous studies, we measured the decision to practice multiplication problems (or in the case of the control condition, complete subtraction problems), along with the number of rounds of practice participants completed. Here, the number of rounds of practice was encoded as follows: participants who did not choose to practice had a value of zero for this variable, participants who chose to practice had a value of one for this variable, and thereafter the variable increased incrementally in correspondence with each round of practice participants completed. This variable had a mean of `r mean(clean$total_practice_rounds_count, na.rm = T)`, with an *SD* of `r sd(clean$total_practice_rounds_count, na.rm = T)`.  

Afterwards, all participants chose a payment scheme for the multiplication task, described and counterbalanced in the same way as in Studies 1 and 2. Then, participants completed the paid multiplication task for two minutes. We included many of the same follow-up questions as in Studies 1 and 2, including measures of risk attitudes, confidence, and perceptions of gender differences in preparation (for participants in the preparation condition), competitiveness, and performance. Notably, we added in an additional response option for all of the questions about perceptions of gender differences in behavior, such that participants in this study could choose between saying men were more likely to engage in the specified behavior, women were more likely to engage in the specified behavior, or that there would be no gender differences in the specified behavior. Like before, participants were incentivized to answer the questions about their confidence and perceptions of gender differences correctly, and were paid at the same rate as Studies 1 and 2. Participants also completed a manipulation check, where they were told about the two conditions, and were asked which of the conditions they thought was more helpful in boosting scores on the paid multiplication task. Finally, they completed some demographic questions and provided feedback on the study before being paid for their participation. 